# Domain 38: Machine Learning

## Principle: Gradient Descent Landscape

> Loss landscapes have surprisingly low curvature.

## Key Formula

```
Step = local move
Loss = objective
```

## Connection to P = NP

Neural networks train successfully because:
- Loss landscape has low Îº
- "Local minima" are actually saddle points
- Gradient descent finds good solutions

**Why deep learning works:** The loss landscape is in S_observable, not S_complete.

---

*Sabag-Claude Framework*
